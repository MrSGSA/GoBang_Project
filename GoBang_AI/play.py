import torch
import numpy as np
from rule import game_rule
from model import game_net
import os
import pickle

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")


def print_board(board):
    """åœ¨ç»ˆç«¯æ‰“å°15x15äº”å­æ£‹æ£‹ç›˜"""
    size = board.shape[0]
    print("\n   ", end="")
    for j in range(size):
        print(f"{j:2d} ", end="")
    print()
    for i in range(size):
        print(f"{i:2d} ", end="")
        for j in range(size):
            if board[i, j] == 1:
                print(" â— ", end="")  # é»‘å­
            elif board[i, j] == -1:
                print(" â—‹ ", end="")  # ç™½å­
            else:
                print(" Â· ", end="")
        print()
    print()


def human_move(env):
    """è·å–äººç±»ç©å®¶è¾“å…¥"""
    while True:
        try:
            inp = input("Your move (row col), e.g. '7 7', or 'q' to quit: ").strip()
            if inp.lower() == "q":
                print("Thanks for playing!")
                exit(0)
            parts = inp.split()
            if len(parts) != 2:
                raise ValueError
            row, col = int(parts[0]), int(parts[1])
            if not (0 <= row < env.size and 0 <= col < env.size):
                print(f"Row and col must be between 0 and {env.size - 1}.")
                continue
            action = row * env.size + col
            if action in env.get_valid_actions():
                return action
            else:
                print("That position is already occupied!")
        except (ValueError, KeyboardInterrupt):
            print("Invalid input. Please enter two numbers like '7 7'.")


def is_winning_move(board, x, y, player, size=15):
    """æ£€æŸ¥åœ¨ (x,y) è½å­åæ˜¯å¦å½¢æˆäº”è¿ï¼ˆæ”¯æŒæ–œå‘ï¼‰"""
    directions = [(1, 0), (0, 1), (1, 1), (1, -1)]
    for dx, dy in directions:
        count = 1  # å½“å‰å­
        # æ­£æ–¹å‘
        for step in range(1, 5):
            nx, ny = x + step * dx, y + step * dy
            if 0 <= nx < size and 0 <= ny < size and board[nx, ny] == player:
                count += 1
            else:
                break
        # åæ–¹å‘
        for step in range(1, 5):
            nx, ny = x - step * dx, y - step * dy
            if 0 <= nx < size and 0 <= ny < size and board[nx, ny] == player:
                count += 1
            else:
                break
        if count >= 5:
            return True
    return False


def ai_move(model, env, human_player):
    """AI ä¸‹ä¸€æ­¥ï¼šå…ˆé˜²èƒœæ‹›ï¼Œå†æŒ‰ç­–ç•¥èµ°"""
    model.eval()
    valid_actions = env.get_valid_actions()

    # ğŸ”’ é˜²å®ˆï¼šæ£€æŸ¥äººç±»ä¸‹ä¸€æ­¥æ˜¯å¦èƒ½èµ¢
    for action in valid_actions:
        temp_board = env.board.copy()
        row, col = action // env.size, action % env.size
        temp_board[row, col] = human_player
        if is_winning_move(temp_board, row, col, human_player, env.size):
            return action

    # ğŸ§  å¦åˆ™ä½¿ç”¨æ¨¡å‹ç­–ç•¥ï¼ˆè´ªå¿ƒï¼‰
    with torch.no_grad():
        state_tensor = (
            torch.tensor(env.board, dtype=torch.float32)
            .unsqueeze(0)
            .unsqueeze(0)
            .to(DEVICE)
        )
        policy_logits, _ = model(state_tensor)
        policy = torch.softmax(policy_logits, dim=1).cpu().numpy()[0]

    # åªè€ƒè™‘åˆæ³•åŠ¨ä½œ
    mask = np.zeros_like(policy)
    mask[valid_actions] = 1.0
    policy *= mask

    if policy.sum() > 0:
        return int(np.argmax(policy))
    else:
        return valid_actions[0]


def save_human_game(states, actions, winner, human_player, filename="human_games.pkl"):
    """ä¿å­˜æ•´å±€å¯¹å±€ä¸ºè®­ç»ƒæ•°æ®"""
    data = []
    for i, (s, a) in enumerate(zip(states, actions)):
        current_player = 1 if i % 2 == 0 else -1
        value = winner * current_player
        data.append((s.astype(np.float32), a, value))

    # è¿½åŠ åˆ°æ–‡ä»¶
    existing = []
    if os.path.exists(filename):
        with open(filename, "rb") as f:
            existing = pickle.load(f)
    existing.extend(data)
    with open(filename, "wb") as f:
        pickle.dump(existing, f)
    print(f"âœ… Game saved! Added {len(data)} samples to '{filename}'.")


def main():
    # åŠ è½½æ¨¡å‹
    model_path = "gomoku_final.pth"
    if not os.path.exists(model_path):
        print(f"âŒ Error: '{model_path}' not found!")
        print("Please train the model first or place it in this directory.")
        return

    model = game_net().to(DEVICE)
    model.load_state_dict(torch.load(model_path, map_location=DEVICE))
    print("âœ… Loaded trained model!")

    # åˆå§‹åŒ–ç¯å¢ƒ
    env = game_rule()
    print("\nğŸ® Welcome to Gomoku vs AI!")
    print("â— = Black (first), â—‹ = White")
    print("Board size: 15Ã—15\n")

    # é€‰æ‹©æ‰§æ–¹
    while True:
        choice = (
            input("Play as Black (â—, first) or White (â—‹, second)? [b/w]: ")
            .strip()
            .lower()
        )
        if choice in ["b", "black"]:
            human_player = 1
            print("You are Black. You go first.")
            break
        elif choice in ["w", "white"]:
            human_player = -1
            print("You are White. AI goes first.")
            break
        else:
            print("Please enter 'b' or 'w'.")

    # å¼€å§‹æ¸¸æˆ

    env.reset()
    states, actions = [], []

    while True:
        print_board(env.board)

        # ç¡®å®šå½“å‰è¯¥è°èµ°ï¼ˆä» env è·å–ï¼Œä¸è¦è‡ªå·±ç®—ï¼ï¼‰
        current_player = env.current_player

        if current_player == human_player:
            action = human_move(env)
            # ğŸ‘‡ å…ˆè®°å½•â€œè½å­å‰â€çš„çŠ¶æ€ï¼ˆç”¨äºè®­ç»ƒï¼‰
            states.append(env.board.copy())
            actions.append(action)
            _, _, done = env.step(action)  # è½å­ + è‡ªåŠ¨åˆ‡æ¢ç©å®¶
        else:
            action = ai_move(model, env, human_player)
            # ğŸ‘‡ åŒæ ·è®°å½•â€œè½å­å‰â€çš„çŠ¶æ€
            states.append(env.board.copy())
            actions.append(action)
            row, col = action // env.size, action % env.size
            _, _, done = env.step(action)
            print(f"AI played at ({row}, {col})")

        if done:
            print_board(env.board)
            winner = env.winner if env.winner is not None else 0
            if winner == human_player:
                print("ğŸ‰ You won! Great job!")
            elif winner == -human_player:
                print("ğŸ’€ AI wins. Better luck next time!")
            else:
                print("ğŸ¤ It's a draw!")

            # ä¿å­˜å¯¹å±€ï¼švalue = winner * player_at_that_step
            data = []
            for i, (s, a) in enumerate(zip(states, actions)):
                # ç¬¬ i æ­¥çš„ç©å®¶ï¼šé»‘å…ˆæ‰‹ â†’ i=0 æ˜¯é»‘(1), i=1 æ˜¯ç™½(-1)...
                player_at_step = 1 if i % 2 == 0 else -1
                value = winner * player_at_step
                data.append((s.astype(np.float32), a, value))

            # ä¿å­˜
            existing = []
            filename = "human_games.pkl"
            if os.path.exists(filename):
                with open(filename, "rb") as f:
                    existing = pickle.load(f)
            existing.extend(data)
            with open(filename, "wb") as f:
                pickle.dump(existing, f)
            print(f"âœ… Game saved! Added {len(data)} samples.")

            break


if __name__ == "__main__":
    main()
