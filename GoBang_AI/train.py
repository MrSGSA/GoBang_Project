# train.py
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import random
import os
from collections import deque

from rule import game_rule  # å‡è®¾è¿™æ˜¯ä½ çš„æ£‹ç›˜é€»è¾‘
from model import game_net  # å‡è®¾è¿™æ˜¯ä½ çš„ç½‘ç»œ
from mcts import MCTS  # è¿™æ˜¯ä¿®æ”¹åçš„ MCTS

# --- å…¨å±€é…ç½® ---
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
BOARD_SIZE = 15
BUFFER_CAPACITY = 30000  # å¢å¤§ Bufferï¼Œé˜²æ­¢é—å¿˜
BATCH_SIZE = 128  # å¢å¤§ Batch sizeï¼Œæ¢¯åº¦æ›´ç¨³
LR = 2e-4  # ç¨å¾®è°ƒé«˜ä¸€ç‚¹
L2_REG = 1e-4
CHECKPOINT_FREQ = 1


class ReplayBuffer:
    def __init__(self, capacity):
        self.buffer = deque(maxlen=capacity)

    def push(self, data):
        self.buffer.extend(data)

    def sample(self, batch_size):
        return random.sample(self.buffer, batch_size)

    def __len__(self):
        return len(self.buffer)


def get_equi_data(play_data):
    """
    æ•°æ®å¢å¼ºï¼šåˆ©ç”¨æ—‹è½¬å’Œç¿»è½¬ï¼Œå°† 1 æ¡æ•°æ®æ‰©å……ä¸º 8 æ¡
    play_data: list of (state, probs, value)
    """
    extended_data = []
    for state, mcts_prob, winner in play_data:
        # state: [15, 15]
        # mcts_prob: [225] -> è¿˜åŸæˆ [15, 15] ç”¨äºå‡ ä½•å˜æ¢
        prob_img = mcts_prob.reshape(BOARD_SIZE, BOARD_SIZE)

        for i in [0, 1, 2, 3]:  # æ—‹è½¬ 0, 90, 180, 270 åº¦
            # 1. æ—‹è½¬
            rot_state = np.rot90(state, i)
            rot_prob = np.rot90(prob_img, i)

            # æ·»åŠ æ—‹è½¬åçš„æ•°æ®
            extended_data.append((rot_state, rot_prob.flatten(), winner))

            # 2. ç¿»è½¬ (åœ¨æ—‹è½¬çš„åŸºç¡€ä¸Šè¿›è¡Œå·¦å³ç¿»è½¬)
            flip_state = np.fliplr(rot_state)
            flip_prob = np.fliplr(rot_prob)

            # æ·»åŠ ç¿»è½¬åçš„æ•°æ®
            extended_data.append((flip_state, flip_prob.flatten(), winner))

    return extended_data


def self_play(model, env, mcts, num_games=1):
    data = []
    model.eval()

    for i in range(num_games):
        env.reset()
        mcts.reset_player()  # é‡ç½® MCTS æ ‘
        states, mcts_probs, current_players = [], [], []

        while True:
            # è·å–å½“å‰ç©å®¶ ID (1 æˆ– -1)
            # å‡è®¾ rule.py ä¸­ steps è®¡æ•°ï¼Œå¶æ•°æ­¥æ˜¯é»‘(1)ï¼Œå¥‡æ•°æ­¥æ˜¯ç™½(-1)
            player = 1 if len(env.steps) % 2 == 0 else -1

            # MCTS æœç´¢
            # temp: å‰å‡ æ­¥æ¸©åº¦é«˜ä¸€ç‚¹ï¼Œå¢åŠ æ¢ç´¢ï¼›åé¢æ¸©åº¦é™ä½ï¼Œé€‰æœ€å¥½çš„
            temp = 1.0 if len(env.steps) < 8 else 1e-3
            action, action_probs = mcts.get_action(env, temp=temp, return_prob=1)

            # --- ğŸ”¥ å…³é”®ï¼šå­˜å…¥ canonical state (å½“å‰ç©å®¶è§†è§’) ---
            # å¦‚æœå½“å‰æ˜¯ç™½æ£‹(-1)ï¼Œå­˜è¿›å»çš„ç›˜é¢è¦ä¹˜ä»¥ -1ï¼Œå˜æˆ "1ä»£è¡¨å·±æ–¹"
            states.append(env.board * player)
            mcts_probs.append(action_probs)
            current_players.append(player)

            # æ‰§è¡ŒåŠ¨ä½œ
            env.step(action)

            winner, end = env.has_a_winner()
            if end:
                # winner: 1(é»‘èƒœ), -1(ç™½èƒœ), 0(å¹³)
                # ä¸ºæ¯ä¸€æ­¥åˆ†é… Value
                winners_z = np.zeros(len(current_players))
                if winner != 0:
                    for j, p in enumerate(current_players):
                        # å¦‚æœ winner == p (è¿™ä¸€æ­¥çš„ç©å®¶èµ¢äº†)ï¼Œåˆ™ v = +1
                        # å¦‚æœ winner != p (è¿™ä¸€æ­¥çš„ç©å®¶è¾“äº†)ï¼Œåˆ™ v = -1
                        winners_z[j] = 1.0 if winner == p else -1.0

                # æ‰“åŒ…è¿™ä¸€å±€çš„æ•°æ®
                data.extend(get_equi_data(zip(states, mcts_probs, winners_z)))
                break
    return data


def evaluate_network(model, env, mcts, num_games=10):
    """
    è¯„ä¼°ï¼šå½“å‰æ¨¡å‹ vs çº¯ MCTS (æˆ–å¼±ä¸€ç‚¹çš„æ—§æ¨¡å‹)
    è¿™é‡Œç®€å•èµ·è§ï¼Œåš MCTS vs Random æˆ–è€… MCTS (Model) vs MCTS (Weak)
    """
    model.eval()
    mcts_sims = 100  # è¯„ä¼°æ—¶ä¸éœ€è¦å¤ªæ·±ï¼Œé€Ÿåº¦ä¼˜å…ˆ
    wins = 0

    for i in range(num_games):
        env.reset()
        mcts.reset_player()
        mcts.set_simulations(mcts_sims)  # ä¸´æ—¶è°ƒæ•´æ¨¡æ‹Ÿæ¬¡æ•°

        model_player = 1 if i % 2 == 0 else -1  # è½®æµæ‰§é»‘

        while True:
            player = 1 if len(env.steps) % 2 == 0 else -1

            if player == model_player:
                # æ¨¡å‹èµ°æ£‹ (ä½æ¸©åº¦ï¼Œè¿½æ±‚æœ€å¼º)
                action = mcts.get_action(env, temp=1e-3)
            else:
                # å¯¹æ‰‹èµ°æ£‹ (è¿™é‡Œç”¨éšæœºä½œä¸ºåŸºå‡†ï¼Œæˆ–è€…å¼± MCTS)
                valid_moves = env.get_valid_actions()
                action = random.choice(valid_moves)

            env.step(action)
            winner, end = env.has_a_winner()
            if end:
                if winner == model_player:
                    wins += 1
                break
    return wins / num_games


def train_cycle(start_epoch=0):
    # åˆå§‹åŒ–
    env = game_rule()
    model = game_net().to(DEVICE)

    # åŠ è½½æ¨¡å‹
    if start_epoch > 0:
        model_path = f"gomoku_model_epoch{start_epoch}.pth"
        if os.path.exists(model_path):
            model.load_state_dict(torch.load(model_path))
            print(f"Loaded {model_path}")

    optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=L2_REG)
    replay_buffer = ReplayBuffer(BUFFER_CAPACITY)

    # åˆå§‹åŒ– MCTS
    # c_puct: æ¢ç´¢å¸¸æ•°ï¼Œé€šå¸¸ 5.0
    mcts = MCTS(model, c_puct=5, num_simulations=400, device=DEVICE)

    for epoch in range(start_epoch, 5000):
        print(f"Epoch {epoch + 1} | Buffer: {len(replay_buffer)}")

        # 1. åŠ¨æ€è°ƒæ•´å‚æ•°
        if epoch < 20:
            games_num, sims, train_steps = 10, 100, 100
        elif epoch < 50:
            games_num, sims, train_steps = 10, 200, 200
        else:
            games_num, sims, train_steps = 10, 400, 300

        # è®¾ç½® MCTS æ¨¡æ‹Ÿæ¬¡æ•°
        mcts.set_simulations(sims)

        # 2. è‡ªæˆ‘å¯¹å¼ˆæ”¶é›†æ•°æ®
        new_data = self_play(model, env, mcts, num_games=games_num)
        replay_buffer.push(new_data)

        # 3. è®­ç»ƒ
        if len(replay_buffer) > BATCH_SIZE:
            model.train()
            loss_sum = 0
            for _ in range(train_steps):
                batch = replay_buffer.sample(BATCH_SIZE)
                # è§£åŒ…æ•°æ®
                state_batch = torch.FloatTensor(np.array([d[0] for d in batch])).to(DEVICE).unsqueeze(
                    1)  # [B, 1, 15, 15]
                mcts_probs_batch = torch.FloatTensor(np.array([d[1] for d in batch])).to(DEVICE)
                winner_batch = torch.FloatTensor(np.array([d[2] for d in batch])).to(DEVICE).unsqueeze(1)

                optimizer.zero_grad()
                # å‰å‘ä¼ æ’­
                log_act_probs, value = model(state_batch)

                # Loss è®¡ç®—
                # Value Loss (MSE)
                value_loss = nn.MSELoss()(value, winner_batch)
                # Policy Loss (Cross Entropy) - æ³¨æ„ mcts_probs æ˜¯æ¦‚ç‡ï¼Œæ¨¡å‹è¾“å‡ºæ˜¯ log_softmax
                # æ‰‹åŠ¨è®¡ç®—äº¤å‰ç†µ: -sum(target * log_pred)
                policy_loss = -torch.mean(torch.sum(mcts_probs_batch * log_act_probs, dim=1))

                loss = value_loss + policy_loss
                loss.backward()
                optimizer.step()
                loss_sum += loss.item()

            print(f"  Loss: {loss_sum / train_steps:.4f}")

        # 4. ä¿å­˜ä¸è¯„ä¼°
        if (epoch + 1) % CHECKPOINT_FREQ == 0:
            torch.save(model.state_dict(), f"gomoku_model_epoch{epoch + 1}.pth")

        if (epoch + 1) % 10 == 0:
            win_rate = evaluate_network(model, env, mcts)
            print(f"  ğŸ“Š Win Rate vs Random: {win_rate:.2%}")


if __name__ == "__main__":
    train_cycle(0)