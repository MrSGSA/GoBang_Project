# mcts.py
import torch
import numpy as np
import copy
import math


class TreeNode:
    def __init__(self, parent, prior_p):
        self.parent = parent
        self.children = {}
        self.n_visits = 0
        self.Q = 0
        self.u = 0
        self.P = prior_p

    def expand(self, action_priors):
        for action, prob in action_priors:
            if action not in self.children:
                self.children[action] = TreeNode(self, prob)

    def select(self, c_puct):
        return max(self.children.items(),
                   key=lambda act_node: act_node[1].get_value(c_puct))

    def get_value(self, c_puct):
        self.u = (c_puct * self.P * math.sqrt(self.parent.n_visits) / (1 + self.n_visits))
        return self.Q + self.u

    def update(self, leaf_value):
        self.n_visits += 1
        self.Q += (leaf_value - self.Q) / self.n_visits
        if self.parent:
            self.parent.update(-leaf_value)

    def is_leaf(self):
        return len(self.children) == 0


class MCTS:
    def __init__(self, policy_value_fn, c_puct=5, num_simulations=100, device="cpu"):
        self.policy_value_fn = policy_value_fn
        self.c_puct = c_puct
        self.num_simulations = num_simulations
        self.device = device
        self.root = TreeNode(None, 1.0)  # åˆå§‹åŒ–æ ¹èŠ‚ç‚¹

    def set_simulations(self, n):
        """åŠ¨æ€è°ƒæ•´æ¨¡æ‹Ÿæ¬¡æ•°"""
        self.num_simulations = n

    def reset_player(self):
        """æ¯å±€å¼€å§‹å‰é‡ç½®æœç´¢æ ‘"""
        self.root = TreeNode(None, 1.0)

    def _playout(self, env):
        node = self.root
        while not node.is_leaf():
            action, node = node.select(self.c_puct)
            env.step(action)

        winner, is_end = env.has_a_winner()

        if is_end:
            # æ¸¸æˆç»“æŸï¼Œwinner æ˜¯ä¸Šä¸€æ­¥èµ°æ£‹çš„äººï¼ˆå³å½“å‰èŠ‚ç‚¹çš„ parentï¼‰
            # å¯¹äºå½“å‰ç­‰å¾…è½å­çš„äººæ¥è¯´ï¼Œå¦‚æœ winner != 0ï¼Œè¯´æ˜ä»–è¾“äº† -> -1
            if winner == 0:
                leaf_value = 0.0
            else:
                leaf_value = -1.0
            node.update(leaf_value)
            return

        # --- è§†è§’è½¬æ¢ (Canonical Form) ---
        current_player = 1 if len(env.steps) % 2 == 0 else -1
        canonical_board = env.board * current_player

        state_tensor = torch.from_numpy(canonical_board).float().to(self.device).unsqueeze(0).unsqueeze(0)

        with torch.no_grad():
            # è¿™é‡Œå¯¹åº” model.py è¾“å‡ºçš„ log_softmax
            log_action_probs, leaf_value = self.policy_value_fn(state_tensor)

        leaf_value = leaf_value.item()
        # å°† log_softmax è¿˜åŸä¸ºæ¦‚ç‡
        action_probs = np.exp(log_action_probs.cpu().numpy().flatten())

        valid_actions = env.get_valid_actions()
        probs = action_probs[valid_actions]

        # å½’ä¸€åŒ–
        probs_sum = np.sum(probs)
        if probs_sum > 0:
            probs /= probs_sum
        else:
            probs = np.ones(len(valid_actions)) / len(valid_actions)

        node.expand(zip(valid_actions, probs))
        node.update(leaf_value)

    def get_action(self, env, temp=1e-3, return_prob=False):
        """
        ğŸ”¥ æ ¸å¿ƒä¿®æ”¹ï¼šå°†åŸæœ¬çš„ run æ”¹åä¸º get_actionï¼Œå¹¶æ”¯æŒè¿”å›æ¦‚ç‡åˆ†å¸ƒ
        """
        # 1. å¦‚æœæ˜¯è®­ç»ƒé˜¶æ®µ(temp>0)ï¼Œæ·»åŠ æ ¹èŠ‚ç‚¹å™ªå£°
        if temp > 0 and self.root.is_leaf():
            # ç¡®ä¿æ ¹èŠ‚ç‚¹å·²å±•å¼€
            sim_env = copy.deepcopy(env)
            self._playout(sim_env)

            if self.root.children:
                actions = list(self.root.children.keys())
                noise = np.random.dirichlet([0.3] * len(actions))
                epsilon = 0.25
                for i, action in enumerate(actions):
                    self.root.children[action].P = (1 - epsilon) * self.root.children[action].P + epsilon * noise[i]

        # 2. æ‰§è¡Œæ¨¡æ‹Ÿ
        for _ in range(self.num_simulations):
            simulation_env = copy.deepcopy(env)
            self._playout(simulation_env)

        # 3. ç»Ÿè®¡è®¿é—®æ¬¡æ•°
        act_visits = [(act, node.n_visits) for act, node in self.root.children.items()]
        if not act_visits:
            # å¼‚å¸¸å…œåº•ï¼šéšæœºè½å­
            action = np.random.choice(env.get_valid_actions())
            probs = np.zeros(env.width * env.height)
            probs[action] = 1.0
            return (action, probs) if return_prob else action

        acts, visits = zip(*act_visits)
        visits = np.array(visits)

        # 4. è®¡ç®—æ¦‚ç‡åˆ†å¸ƒ
        if temp == 0:
            # è´ªå©ªæ¨¡å¼ (Evaluation)
            best_idx = np.argmax(visits)
            action = acts[best_idx]
            # æ„é€  one-hot æ¦‚ç‡ï¼ˆä¸ºäº†æ ¼å¼ç»Ÿä¸€ï¼‰
            act_probs = np.zeros(len(acts))
            act_probs[best_idx] = 1.0
        else:
            # é‡‡æ ·æ¨¡å¼ (Self-play)
            # é˜²æ­¢ temp è¿‡å°å¯¼è‡´æº¢å‡º
            if temp < 1e-3:
                temp = 1e-3

                # ä½¿ç”¨ softmax é£æ ¼çš„æ¸©åº¦è°ƒèŠ‚ï¼Œæˆ–è€…ç›´æ¥ visits^(1/temp)
            # AlphaZero æ ‡å‡†åšæ³•æ˜¯ visits^(1/temp) / sum
            visits_temp = visits ** (1.0 / temp)
            act_probs = visits_temp / np.sum(visits_temp)
            action = np.random.choice(acts, p=act_probs)

        # 5. MCTS æ ‘å¤ç”¨ (Move root)
        if action in self.root.children:
            self.root = self.root.children[action]
            self.root.parent = None
        else:
            self.root = TreeNode(None, 1.0)

        # 6. è¿”å›ç»“æœ
        if return_prob:
            # æ„é€ å®Œæ•´çš„ 15x15 æ¦‚ç‡å‘é‡ (225,)
            full_probs = np.zeros(env.width * env.height)
            full_probs[list(acts)] = act_probs
            return action, full_probs
        else:
            return action