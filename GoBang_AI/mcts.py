# mcts.py
import torch
import numpy as np
import copy
import math


class TreeNode:
    def __init__(self, parent, prior_p):
        self.parent = parent
        self.children = {}  # {action: TreeNode}
        self.n_visits = 0
        self.Q = 0
        self.u = 0
        self.P = prior_p

    def expand(self, action_priors):
        """æ‰©å±•å­èŠ‚ç‚¹"""
        for action, prob in action_priors:
            if action not in self.children:
                self.children[action] = TreeNode(self, prob)

    def select(self, c_puct):
        """
        é€‰æ‹© UCB å€¼æœ€å¤§çš„å­èŠ‚ç‚¹
        UCB = Q + c_puct * P * sqrt(parent_visits) / (1 + child_visits)
        """
        return max(self.children.items(),
                   key=lambda act_node: act_node[1].get_value(c_puct))

    def get_value(self, c_puct):
        """è®¡ç®— Upper Confidence Bound (UCB)"""
        # åŠ ä¸Š 1e-10 é˜²æ­¢é™¤ä»¥ 0ï¼ˆè™½ç„¶é€šå¸¸ n_visits åˆå§‹ä¸º0æ—¶å…¬å¼èƒ½å¤„ç†ï¼Œä½†åŠ ä¿é™©ï¼‰
        self.u = (c_puct * self.P * math.sqrt(self.parent.n_visits) / (1 + self.n_visits))
        return self.Q + self.u

    def update(self, leaf_value):
        """
        åå‘ä¼ æ’­æ›´æ–°ä»·å€¼
        leaf_value: å¯¹äºå½“å‰èŠ‚ç‚¹æ‰€å±ç©å®¶çš„ä»·å€¼ (v)
        """
        self.n_visits += 1
        # Q å€¼æ›´æ–°ï¼šç´¯è®¡å¹³å‡
        self.Q += (leaf_value - self.Q) / self.n_visits

        # é€’å½’æ›´æ–°çˆ¶èŠ‚ç‚¹
        # æ³¨æ„ï¼šçˆ¶èŠ‚ç‚¹æ˜¯å¯¹æ‰‹ï¼Œæ‰€ä»¥ä»·å€¼å–å (-leaf_value)
        if self.parent:
            self.parent.update(-leaf_value)

    def is_leaf(self):
        return len(self.children) == 0


class MCTS:
    def __init__(self, policy_value_fn, c_puct=5, num_simulations=100, device="cpu"):
        self.policy_value_fn = policy_value_fn  # ç¥ç»ç½‘ç»œæ¨¡å‹
        self.c_puct = c_puct
        self.num_simulations = num_simulations
        self.device = device
        self.root = None

    def _playout(self, env):
        """æ‰§è¡Œä¸€æ¬¡æ¨¡æ‹Ÿï¼šSelection -> Expansion -> Evaluation -> Backup"""
        node = self.root

        # 1. Selection: ä¸€ç›´èµ°åˆ°å¶å­èŠ‚ç‚¹
        while not node.is_leaf():
            action, node = node.select(self.c_puct)
            env.step(action)

        # æ­¤æ—¶ env å¤„äºå¶å­èŠ‚ç‚¹çŠ¶æ€
        # 2. åˆ¤æ–­æ¸¸æˆæ˜¯å¦ç»“æŸ
        winner, is_end = env.has_a_winner()  # å»ºè®® rule.py ç»Ÿä¸€è¿”å› (winner, end_flag)
        # å¦‚æœä½ çš„ rule.py åªæœ‰ env.winner å’Œ env.boardï¼Œå¯ä»¥ç”¨ä¸‹é¢çš„é€»è¾‘ï¼š
        # winner = env.winner
        # is_end = (winner != 0) or np.all(env.board != 0)

        if is_end:
            if winner == 0:
                leaf_value = 0.0
            else:
                # æ¸¸æˆç»“æŸä¸”æœ‰èµ¢å®¶ã€‚
                # ç”±äºè¿™æ˜¯é€šè¿‡ step è¿›å…¥çš„èŠ‚ç‚¹ï¼Œè¯´æ˜ä¸Šä¸€æ­¥èµ°æ£‹çš„äººèµ¢äº†ã€‚
                # ä¹Ÿå°±æ˜¯å½“å‰èŠ‚ç‚¹å¯¹åº”çš„ç©å®¶ï¼ˆç­‰å¾…è½å­çš„äººï¼‰è¾“äº†ã€‚
                # æ‰€ä»¥å¯¹äºå½“å‰èŠ‚ç‚¹ç©å®¶ï¼Œä»·å€¼æ˜¯ -1ã€‚
                leaf_value = -1.0

                # åå‘ä¼ æ’­ (æ³¨æ„ï¼šupdateå†…éƒ¨ä¼šè‡ªåŠ¨å¤„ç†çˆ¶èŠ‚ç‚¹çš„å–å)
            node.update(leaf_value)
            return

        # 3. Expansion & Evaluation (é€šè¿‡ç¥ç»ç½‘ç»œ)

        # --- ğŸ”¥ å…³é”®ä¿®æ”¹ï¼šè§†è§’è½¬æ¢ (Canonical Form) ---
        # å¿…é¡»æŠŠç›˜é¢è½¬æ¢æˆâ€œå½“å‰ç©å®¶æ˜¯é»‘æ£‹(1)â€çš„è§†è§’
        # å‡è®¾ steps é•¿åº¦ä¸ºå¶æ•°æ˜¯é»‘æ£‹å›åˆï¼Œå¥‡æ•°æ˜¯ç™½æ£‹å›åˆ
        current_player = 1 if len(env.steps) % 2 == 0 else -1
        canonical_board = env.board * current_player

        # è½¬æ¢ä¸º Tensor
        state_tensor = torch.from_numpy(canonical_board).float().to(self.device).unsqueeze(0).unsqueeze(0)

        with torch.no_grad():
            # å‡è®¾æ¨¡å‹è¾“å‡ºæ˜¯ log_softmax(act) å’Œ tanh(val)
            log_action_probs, leaf_value = self.policy_value_fn(state_tensor)

        # å¤„ç† Value
        leaf_value = leaf_value.item()  # [-1, 1]

        # å¤„ç† Policy
        # å› ä¸ºæ¨¡å‹è¾“å‡ºæ˜¯ log_softmaxï¼Œæˆ‘ä»¬éœ€è¦ exp å˜å›æ¦‚ç‡
        action_probs = np.exp(log_action_probs.cpu().numpy().flatten())

        # è·å–åˆæ³•åŠ¨ä½œ
        valid_actions = env.get_valid_actions()

        # è¿‡æ»¤å¹¶å½’ä¸€åŒ–æ¦‚ç‡
        probs = action_probs[valid_actions]
        probs_sum = np.sum(probs)
        if probs_sum > 0:
            probs /= probs_sum  # é‡æ–°å½’ä¸€åŒ–
        else:
            # æç½•è§æƒ…å†µï¼šæ¨¡å‹è®¤ä¸ºæ‰€æœ‰åˆæ³•åŠ¨ä½œæ¦‚ç‡éƒ½æå°ï¼Œé€€åŒ–ä¸ºå‡åŒ€åˆ†å¸ƒ
            probs = np.ones(len(valid_actions)) / len(valid_actions)

        # æ‰©å±•èŠ‚ç‚¹
        action_priors = zip(valid_actions, probs)
        node.expand(action_priors)

        # 4. Backup
        node.update(leaf_value)

    def run(self, env, temp=1.0):
        """
        æ‰§è¡Œ MCTS æœç´¢å¹¶è¿”å›åŠ¨ä½œ
        temp: æ¸©åº¦å‚æ•°
        """
        self.root = TreeNode(None, 1.0)

        # --- ğŸ”¥ ä¼˜åŒ–ï¼šå¢åŠ æ ¹èŠ‚ç‚¹å™ªå£° (Dirichlet Noise) ---
        # ä»…åœ¨è®­ç»ƒé˜¶æ®µ(temp > 0)æˆ–ç¡®å®éœ€è¦æ¢ç´¢æ—¶æ·»åŠ 
        # è¿™æœ‰åŠ©äºé˜²æ­¢æ¨¡å‹åœ¨è‡ªæˆ‘åšå¼ˆä¸­è¿‡æ—©æ”¶æ•›åˆ°å•ä¸€ç­–ç•¥
        if temp > 0:
            # å…ˆè·‘ä¸€æ¬¡ç½‘ç»œè·å– Priorï¼Œä»¥ä¾¿åŠ å™ªå£°
            # æ³¨æ„ï¼šè¿™é‡Œä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬é€šå¸¸ä¾èµ–ç¬¬ä¸€æ¬¡ simulation æ¥å±•å¼€æ ¹èŠ‚ç‚¹
            # ä½†ä¸ºäº†åŠ å™ªå£°ï¼Œæˆ‘ä»¬éœ€è¦ç¡®ä¿æ ¹èŠ‚ç‚¹å·²ç»å±•å¼€ã€‚

            # ç®€å•åšæ³•ï¼šå…ˆåšä¸€æ¬¡æ¨¡æ‹Ÿï¼Œç¡®ä¿ root å±•å¼€
            sim_env = copy.deepcopy(env)
            self._playout(sim_env)

            # æ·»åŠ å™ªå£°
            if self.root.children:
                actions = list(self.root.children.keys())
                noise = np.random.dirichlet([0.3] * len(actions))
                epsilon = 0.25  # å™ªå£°æƒé‡ï¼ŒAlphaZero æ ‡å‡†æ˜¯ 0.25

                for i, action in enumerate(actions):
                    node = self.root.children[action]
                    # æ··åˆç½‘ç»œé¢„æµ‹æ¦‚ç‡(P)ä¸å™ªå£°
                    node.P = (1 - epsilon) * node.P + epsilon * noise[i]

        # å¼€å§‹æ­£å¼æ¨¡æ‹Ÿ
        for _ in range(self.num_simulations):
            simulation_env = copy.deepcopy(env)
            self._playout(simulation_env)

        # è®¡ç®—æ¯ä¸ªåŠ¨ä½œçš„è®¿é—®æ¬¡æ•°
        counts = [(act, node.n_visits) for act, node in self.root.children.items()]

        if not counts:
            # å¼‚å¸¸ä¿æŠ¤ï¼šå¦‚æœæ²¡æœ‰åˆæ³•åŠ¨ä½œï¼ˆè™½ç„¶ playout åº”è¯¥å¤„ç†äº†ï¼‰
            return np.random.choice(env.get_valid_actions())

        acts, visits = zip(*counts)
        visits = np.array(visits)

        if temp == 0:
            # è¯„ä¼°/ç«æŠ€æ¨¡å¼ï¼šè´ªå©ªé€‰æ‹©è®¿é—®é‡æœ€å¤§çš„
            action = acts[np.argmax(visits)]
        else:
            # è®­ç»ƒæ¨¡å¼ï¼šæ ¹æ®è®¿é—®é‡æ¦‚ç‡åˆ†å¸ƒé‡‡æ ·
            # ä¸ºäº†æ•°å€¼ç¨³å®šæ€§
            if temp == 1.0:
                probs = visits / np.sum(visits)
            else:
                # é¿å…è¿‡å¤§çš„ temp å¯¼è‡´æº¢å‡ºï¼Œæˆ–è€…è¿‡å°çš„ temp å¯¼è‡´é™¤é›¶
                visits = visits ** (1.0 / temp)
                probs = visits / np.sum(visits)

            action = np.random.choice(acts, p=probs)

        return action

    def reset_player(self):
        self.root = TreeNode(None, 1.0)

    def set_simulations(self, n):
        self.num_simulations = n